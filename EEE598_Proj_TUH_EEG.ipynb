{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpmadabhu/EEE598_proj_tuh_eeg/blob/main/EEE598_Proj_TUH_EEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUSZ Preprocessing Pipeline\n",
        "\n",
        "This notebook builds a seizure vs background window dataset from TUH Seizure Corpus (TUSZ) tcp-bipolar montage EEG.\n",
        "\n",
        "- Recursively walks `train/`, `dev/`, or `eval` under `ROOT_DIR`.\n",
        "- For each EDF:\n",
        "  - Load EEG + its seizure annotations (.csv).\n",
        "  - Bandpass 0.5–40 Hz using a Kaiser FIR, zero-phase (`filtfilt`).\n",
        "  - Downsample to 250 Hz *if* original fs is higher; never upsample.\n",
        "  - Robust z-score per channel (median/IQR).\n",
        "  - Slice into 10 s windows with 5 s hop.\n",
        "  - Label each window seizure(1) or background(0), pooling all seizure subtypes.\n",
        "- Skips recordings that are too short for `filtfilt` padding.\n",
        "- Saves:\n",
        "  - `split_windows.npz` with windows, labels, and metadata.\n",
        "  - `qc_window.png` plotting an informative example window.\n",
        "  - `class_balance.png` showing seizure/background counts.\n",
        "\n",
        "## Smart Canonical Montage\n",
        "We do **not** force a giant hard-coded canonical bipolar channel list upfront.\n",
        "Instead we learn a canonical layout from *inside the split*:\n",
        "\n",
        "1. The **first usable recording** becomes our **reference layout**.\n",
        "   - We store its channel names/ordering. Call this `ref_ch_names`.\n",
        "2. Every later recording is aligned to that same reference layout:\n",
        "   - For each reference channel name (like `FP1-F7`), we try to find the best-matching channel name in the new recording using a robust normalizer.\n",
        "   - If found, we copy that channel.\n",
        "   - If missing, we fill zeros in that row.\n",
        "3. We compute a **coverage ratio** = fraction of reference channels we could actually fill with real data from this recording.\n",
        "   - If coverage is too low (default <30%), we SKIP that recording to avoid mostly-zero data.\n",
        "\n",
        "\n",
        "## Runtime knobs\n",
        "- `MAX_SUBJECTS`: cap how many subjects to include (fast debug).\n",
        "- `MAX_RECORDINGS`: cap recordings per subject (or overall).\n",
        "- `NUM_TAPS`: FIR length. Shorter => fewer clips skipped for being \"too short\".\n",
        "- `GROUP_MODE`: iterate by `\"subject\"` or just all EDFs (`\"recording\"`).\n",
        "- `min_coverage`: skip a recording if < this fraction of reference channels could be mapped."
      ],
      "metadata": {
        "id": "FiN7a_NFCib2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6_pj_p4yc6D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import mne\n",
        "from scipy.signal import firwin, filtfilt, resample_poly\n",
        "from math import gcd\n",
        "import io, re\n",
        "import random\n",
        "\n",
        "# ------------------ USER CONFIG ------------------\n",
        "# Point this at the root of the TUSZ split folders that contain train/dev/eval\n",
        "ROOT_DIR = Path(\"Enter the mounted onedrive location of the edf folder of tusz database\").expanduser().resolve()\n",
        "\n",
        "# Output directory for NPZ + plots\n",
        "OUT_DIR  = Path(\"output folder to save the windows ans coverage stats\").expanduser().resolve()\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Windowing params\n",
        "WIN_LEN_S = 10.0   # seconds per window\n",
        "HOP_S     = 5.0    # seconds hop (50% overlap)\n",
        "\n",
        "# DSP params\n",
        "NUM_TAPS  = 1001    # Kaiser FIR taps for bandpass 0.5-40Hz (shorter -> fewer skips)\n",
        "TARGET_FS = 250.0  # We'll downsample to 250 Hz if fs_orig > 250 Hz; never upsample\n",
        "\n",
        "# Runtime limiting params\n",
        "MAX_SUBJECTS   = 10        # limit number of subjects used (debug speed). None = no explicit cap\n",
        "MAX_RECORDINGS = 10      # max recordings per subject / overall. None = no explicit cap\n",
        "GROUP_MODE     = \"subject\" # \"subject\" or \"recording\"\n",
        "\n",
        "# Seizure subtype labels we consider \"seizure\" (label=1)\n",
        "# Everything else becomes background (label=0)\n",
        "SEIZURE_LABELS = {\n",
        "    'seiz','fnsz','gnsz','spsz','cpsz','absz',\n",
        "    'tnsz','cnsz','tcsz','atsz','mysz'\n",
        "}\n",
        "\n",
        "# Artifact / noise labels in TUSZ annotations.\n",
        "# Based on TUSZ event map, includes muscle, eye movement, chewing, shiver,\n",
        "# electrode pops and combos\n",
        "ARTIFACT_LABELS = {\n",
        "    'artf',        # generic artifact\n",
        "    'eyem',        # eye movement / blink\n",
        "    'chew',        # chewing\n",
        "    'shiv',        # shiver / tremor\n",
        "    'musc',        # muscle artifact\n",
        "    'elec',        # electrode pop / line noise\n",
        "    # common multi-label combos found in TUSZ maps:\n",
        "    'eyem_chew','eyem_shiv','eyem_musc','eyem_elec',\n",
        "    'chew_shiv','chew_musc','chew_elec',\n",
        "    'shiv_musc','shiv_elec',\n",
        "    'musc_elec'\n",
        "}\n",
        "\n",
        "print(\"ROOT_DIR:\", ROOT_DIR)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering / resampling / normalization\n",
        "\n",
        "### Bandpass 0.5–40 Hz (Kaiser FIR)\n",
        "- We design a linear-phase FIR using `firwin(..., window=('kaiser', beta))`.\n",
        "- We run `filtfilt` per channel to get zero-phase.\n",
        "- `filtfilt` needs enough padding, so we skip recordings that are too short.\n",
        "\n",
        "### Resampling\n",
        "- If original fs > 250 Hz, downsample to 250 using polyphase (`resample_poly`).\n",
        "- If fs <= 250 Hz, leave it (no upsampling).\n",
        "\n",
        "### Robust z-score\n",
        "- Per channel: subtract median, divide by IQR.\n",
        "- Flat channels (IQR ≈ 0) become ~0, which is fine."
      ],
      "metadata": {
        "id": "0xzOBY7mDnjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _kaiser_beta_from_ripple(ripple_db: float) -> float:\n",
        "    \"\"\"\n",
        "    Approximate Kaiser beta as a function of desired stopband attenuation in dB.\n",
        "    \"\"\"\n",
        "    A = ripple_db\n",
        "    if A > 50:\n",
        "        return 0.1102 * (A - 8.7)\n",
        "    elif A >= 21:\n",
        "        return 0.5842 * (A - 21)**0.4 + 0.07886 * (A - 21)\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def apply_kaiser_bandpass(data, fs_orig, f_lo=0.5, f_hi=40.0, num_taps=101, ripple_db=60.0):\n",
        "    n_ch, n_samp = data.shape\n",
        "    padlen_needed = 3 * (num_taps - 1)\n",
        "    if n_samp <= padlen_needed:\n",
        "        raise ValueError(\n",
        "            f\"too short for filtfilt len={n_samp} need>{padlen_needed}\"\n",
        "        )\n",
        "\n",
        "    beta = _kaiser_beta_from_ripple(ripple_db)\n",
        "    taps = firwin(\n",
        "        numtaps=num_taps,\n",
        "        cutoff=[f_lo, f_hi],\n",
        "        pass_zero=False,\n",
        "        window=(\"kaiser\", beta),\n",
        "        fs=fs_orig,\n",
        "        scale=True,\n",
        "    )\n",
        "\n",
        "    # zero-phase per channel\n",
        "    filtered = np.stack([filtfilt(taps, [1.0], ch, axis=0) for ch in data], axis=0)\n",
        "    return filtered\n",
        "\n",
        "def maybe_resample_to_250(data, fs_orig, target_fs=TARGET_FS):\n",
        "    if abs(fs_orig - target_fs) < 1e-6:\n",
        "        return data, fs_orig\n",
        "    if fs_orig < target_fs:\n",
        "        print(f\"WARNING: fs {fs_orig} Hz < {target_fs} Hz, not upsampling.\")\n",
        "        return data, fs_orig\n",
        "\n",
        "    # integer ratio for resample_poly\n",
        "    from math import gcd\n",
        "    up = int(target_fs * 1000)\n",
        "    down = int(fs_orig * 1000)\n",
        "    g = gcd(up, down)\n",
        "    up //= g\n",
        "    down //= g\n",
        "\n",
        "    resampled = []\n",
        "    for ch in data:\n",
        "        ch_rs = resample_poly(ch, up, down)\n",
        "        resampled.append(ch_rs)\n",
        "    resampled = np.stack(resampled, axis=0)\n",
        "    return resampled, target_fs\n",
        "\n",
        "def robust_zscore(data, eps=1e-9):\n",
        "    med = np.median(data, axis=1, keepdims=True)\n",
        "    p75 = np.percentile(data, 75, axis=1, keepdims=True)\n",
        "    p25 = np.percentile(data, 25, axis=1, keepdims=True)\n",
        "    iqr = p75 - p25\n",
        "    return (data - med) / (iqr + eps)"
      ],
      "metadata": {
        "id": "r2Jq07S0DyRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotation loader (CSV)\n",
        "\n",
        "TUSZ seizure annotations are in `.csv` alongside the EDF. We:\n",
        "1. Open the `.csv` and try to locate the header line that contains `start` and `stop`.\n",
        "2. Read from that line forward using pandas.\n",
        "3. Extract time intervals for any label in a target label set.\n",
        "\n",
        "We create two label sets:\n",
        "- `SEIZURE_LABELS`: all events we consider “seizure” for model supervision.\n",
        "- `ARTIFACT_LABELS`: all events we consider “artifact / unusable background”, including muscle noise, eye movement, chewing, electrode pops, etc.\n",
        "\n",
        "We then define:\n",
        "- `load_seizure_intervals(csv_path)`:merged `(start_sec, stop_sec)` intervals containing seizure activity.\n",
        "- `load_artifact_intervals(csv_path)`:merged `(start_sec, stop_sec)` intervals containing artifact.\n",
        "\n",
        "Merging step:\n",
        "- If two intervals overlap or touch, we merge them into a single continuous interval.  \n",
        "  This prevents double-counting when annotations are dense.\n",
        "\n",
        "\n",
        "These intervals are later used to decide if each 10 s window is seizure, clean background, or should be thrown away due to artifact."
      ],
      "metadata": {
        "id": "t8BsE3hCEBNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_annotation_csv(csv_path: Path):\n",
        "\n",
        "    csv_path = Path(csv_path)\n",
        "    try:\n",
        "        raw_text = csv_path.read_text(errors=\"replace\")\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not open {csv_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    lines = raw_text.splitlines()\n",
        "    header_idx = None\n",
        "    for i, line in enumerate(lines):\n",
        "        low = line.lower()\n",
        "        if (\"start\" in low) and (\"stop\" in low):\n",
        "            header_idx = i\n",
        "            break\n",
        "\n",
        "    if header_idx is None:\n",
        "        # try naive pandas read\n",
        "        try:\n",
        "            ann = pd.read_csv(\n",
        "                csv_path,\n",
        "                sep=\",\",\n",
        "                engine=\"python\",\n",
        "                on_bad_lines=\"skip\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not parse {csv_path}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        main_body = \"\\n\".join(lines[header_idx:])\n",
        "        try:\n",
        "            ann = pd.read_csv(\n",
        "                io.StringIO(main_body),\n",
        "                sep=\",\",\n",
        "                engine=\"python\",\n",
        "                on_bad_lines=\"skip\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not smart-parse {csv_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    if ann is None or ann.empty:\n",
        "        return None\n",
        "\n",
        "    # normalize column names\n",
        "    ann.columns = [c.strip().lower() for c in ann.columns]\n",
        "    return ann\n",
        "\n",
        "\n",
        "def _extract_intervals_from_ann_df(ann_df: pd.DataFrame,\n",
        "                                   target_label_set: set[str]):\n",
        "\n",
        "    if ann_df is None or ann_df.empty:\n",
        "        return []\n",
        "\n",
        "    # figure out column names for start/stop and label\n",
        "    if 'start_time' in ann_df.columns:\n",
        "        start_col = 'start_time'\n",
        "    elif 'start' in ann_df.columns:\n",
        "        start_col = 'start'\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    if 'stop_time' in ann_df.columns:\n",
        "        stop_col = 'stop_time'\n",
        "    elif 'stop' in ann_df.columns:\n",
        "        stop_col = 'stop'\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    if 'label' in ann_df.columns:\n",
        "        label_col = 'label'\n",
        "    elif 'type' in ann_df.columns:\n",
        "        label_col = 'type'\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    # normalize the label column\n",
        "    tmp = ann_df.copy()\n",
        "    tmp[label_col] = tmp[label_col].astype(str).str.lower().str.strip()\n",
        "\n",
        "    use_rows = tmp[tmp[label_col].isin(target_label_set)]\n",
        "    if use_rows.empty:\n",
        "        return []\n",
        "\n",
        "    raw_intervals = []\n",
        "    for _, r in use_rows.iterrows():\n",
        "        try:\n",
        "            s = float(r[start_col])\n",
        "            e = float(r[stop_col])\n",
        "        except Exception:\n",
        "            continue\n",
        "        if not np.isfinite(s) or not np.isfinite(e):\n",
        "            continue\n",
        "        if e <= s:\n",
        "            continue\n",
        "        raw_intervals.append([s, e])\n",
        "\n",
        "    if not raw_intervals:\n",
        "        return []\n",
        "\n",
        "    # merge overlaps\n",
        "    raw_intervals.sort(key=lambda x: x[0])\n",
        "    merged = [raw_intervals[0]]\n",
        "    for s, e in raw_intervals[1:]:\n",
        "        ls, le = merged[-1]\n",
        "        if s <= le:\n",
        "            merged[-1][1] = max(le, e)\n",
        "        else:\n",
        "            merged.append([s, e])\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "def load_seizure_intervals(csv_path: Path):\n",
        "    ann_df = _parse_annotation_csv(csv_path)\n",
        "    return _extract_intervals_from_ann_df(ann_df, SEIZURE_LABELS)\n",
        "\n",
        "\n",
        "def load_artifact_intervals(csv_path: Path):\n",
        "    ann_df = _parse_annotation_csv(csv_path)\n",
        "    return _extract_intervals_from_ann_df(ann_df, ARTIFACT_LABELS)"
      ],
      "metadata": {
        "id": "6VmdMDMcEGPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sliding window extraction and labeling\n",
        "\n",
        "We cut the filtered, resampled, z-scored EEG into 10 s windows with 5 s hop.\n",
        "\n",
        "For each window `[w_start, w_end]`:\n",
        "1. If it overlaps any **seizure interval**: we KEEP the window and label it as seizure (`label = 1`).\n",
        "   - Seizure **wins** over artifact. We do *not* discard seizure windows even if they contain some artifact. We want to keep seizure evidence.\n",
        "2. Else if it overlaps any **artifact interval** (muscle, chewing, blink, electrode pop, etc.) -> we DROP that window entirely. It will *not* appear in the dataset.\n",
        "   - This prevents noisy or motion-contaminated background from being learned as “normal.”\n",
        "3. Else: we KEEP the window and label it as clean background (`label = 0`).\n",
        "\n",
        "We return:\n",
        "- `X_arr`: `[N_win, C, T]` (Number of windows, Channels and Time Samples per 10s window)\n",
        "- `y_arr`: `[N_win]` seizure/background labels\n",
        "- `t0_arr`: `[N_win]` start time (sec from file start)"
      ],
      "metadata": {
        "id": "A2DGmoIcEkdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_data_with_artifact(data,\n",
        "                              fs,\n",
        "                              seizure_intervals,\n",
        "                              artifact_intervals,\n",
        "                              win_len_s,\n",
        "                              hop_s):\n",
        "\n",
        "    n_ch, n_samp = data.shape\n",
        "    win_len = int(round(win_len_s * fs))\n",
        "    hop     = int(round(hop_s * fs))\n",
        "\n",
        "    X_list  = []\n",
        "    Y_list  = []\n",
        "    T0_list = []\n",
        "\n",
        "    dropped_art = 0\n",
        "\n",
        "    idx = 0\n",
        "    while idx + win_len <= n_samp:\n",
        "        seg = data[:, idx:idx + win_len]\n",
        "        w_start = idx / fs\n",
        "        w_end   = (idx + win_len) / fs\n",
        "\n",
        "        # check seizure overlap\n",
        "        is_seiz = any((w_start < sz_e) and (w_end > sz_s)\n",
        "                      for (sz_s, sz_e) in seizure_intervals)\n",
        "\n",
        "        if is_seiz:\n",
        "            X_list.append(seg)\n",
        "            Y_list.append(1)\n",
        "            T0_list.append(w_start)\n",
        "            idx += hop\n",
        "            continue\n",
        "\n",
        "        # check artifact overlap (only if not seizure)\n",
        "        is_artf = any((w_start < a_e) and (w_end > a_s)\n",
        "                      for (a_s, a_e) in artifact_intervals)\n",
        "\n",
        "        if is_artf:\n",
        "            # drop this window\n",
        "            dropped_art += 1\n",
        "            idx += hop\n",
        "            continue\n",
        "\n",
        "        # clean background\n",
        "        X_list.append(seg)\n",
        "        Y_list.append(0)\n",
        "        T0_list.append(w_start)\n",
        "\n",
        "        idx += hop\n",
        "\n",
        "    if not X_list:\n",
        "        return (\n",
        "            np.empty((0, n_ch, win_len), dtype=np.float32),\n",
        "            np.empty((0,), dtype=np.int64),\n",
        "            np.empty((0,), dtype=np.float32),\n",
        "            dropped_art\n",
        "        )\n",
        "\n",
        "    X_arr  = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y_arr  = np.array(Y_list, dtype=np.int64)\n",
        "    t0_arr = np.array(T0_list, dtype=np.float32)\n",
        "\n",
        "    return X_arr, y_arr, t0_arr, dropped_art"
      ],
      "metadata": {
        "id": "ua_nb1qJFck6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matching EDF ↔ CSV\n",
        "\n",
        "`file.edf` -> `file.csv`"
      ],
      "metadata": {
        "id": "pEcS2iAzFnfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_annotation_for_edf(edf_path: Path):\n",
        "    edf_path = Path(edf_path)\n",
        "    d = edf_path.parent\n",
        "    stem = edf_path.stem\n",
        "\n",
        "    exact = d / f\"{stem}.csv\"\n",
        "    if exact.exists():\n",
        "        return exact\n",
        "\n",
        "    csvs = list(d.glob('*.csv'))\n",
        "    pref = [c for c in csvs if c.stem.startswith(stem)]\n",
        "    if pref:\n",
        "        return pref[0]\n",
        "\n",
        "    if len(csvs) == 1:\n",
        "        return csvs[0]\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "1KKMStMnF7EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smart Canonical Montage: channel normalization + alignment\n",
        "\n",
        "Normalize channel names so variants like:\n",
        "- `EEG FP1-F7`\n",
        "- `fp1_f7-ref`\n",
        "- `FP1–F7`\n",
        "map to the same key: `FP1-F7`.\n",
        "This handles prefixes (`EEG `), suffixes (`-REF`, `-AVG`, etc.), underscores, dashes.\n",
        "\n",
        "For each processed recording *after* filtering/z-scoring/windowing:\n",
        "- We know its own channel names (`dbg['channel_names']`).\n",
        "- We know the **reference channel layout** picked from the *first* good recording `ref_ch_names`.\n",
        "- We build a lookup from normalized channel name -> row index in this recording.\n",
        "- For each ref channel name, if we find a match, we copy that row; if not, we insert zeros.\n",
        "- We track coverage_ratio = fraction of ref channels we successfully filled.\n",
        "If coverage is too low (<30% by default), we skip that recording so we don't poison the dataset with mostly zeros.\n"
      ],
      "metadata": {
        "id": "XyE1zf7GF_Cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _normalize_ch_name(name: str) -> str:\n",
        "\n",
        "    n = name.upper().strip()\n",
        "\n",
        "    # drop leading 'EEG '\n",
        "    n = re.sub(r'^EEG\\s+', '', n)\n",
        "\n",
        "    # normalize fancy dashes to plain '-'\n",
        "    n = n.replace('–', '-').replace('—', '-')\n",
        "\n",
        "    # collapse spaces/underscores into '-'\n",
        "    n = re.sub(r'[\\s_]+', '-', n)\n",
        "\n",
        "    # remove common reference suffixes like -REF, -AVG, -AV, -LE, A1/A2, M1/M2\n",
        "    n = re.sub(r'-(REF|AVG|AV|LE|A1|A2|M1|M2)$', '', n)\n",
        "\n",
        "    # collapse multiple dashes\n",
        "    n = re.sub(r'-+', '-', n)\n",
        "\n",
        "    # trim stray '-'\n",
        "    n = n.strip('- ')\n",
        "\n",
        "    return n\n",
        "\n",
        "def align_to_reference(data_curr, ch_names_curr, ref_ch_names):\n",
        "\n",
        "    norm_lookup = {}\n",
        "    for i, nm in enumerate(ch_names_curr):\n",
        "        norm_lookup[_normalize_ch_name(nm)] = i\n",
        "\n",
        "    T = data_curr.shape[1]\n",
        "    out = np.zeros((len(ref_ch_names), T), dtype=data_curr.dtype)\n",
        "\n",
        "    filled = 0\n",
        "    for i, ref_nm in enumerate(ref_ch_names):\n",
        "        idx = norm_lookup.get(_normalize_ch_name(ref_nm))\n",
        "        if idx is not None:\n",
        "            out[i, :] = data_curr[idx, :]\n",
        "            filled += 1\n",
        "\n",
        "    coverage_ratio = filled / max(1, len(ref_ch_names))\n",
        "    return out, coverage_ratio"
      ],
      "metadata": {
        "id": "hPhZyvSJGZzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess a single EDF (Reference channel format)\n",
        "\n",
        "Steps:\n",
        "1. Load EDF with MNE.\n",
        "2. 0.5–40 Hz bandpass via Kaiser FIR + zero-phase `filtfilt`.\n",
        "3. Downsample to 250 Hz if fs > 250 (never upsample).\n",
        "4. Robust z-score per channel.\n",
        "5. Load seizure intervals from .csv, merge overlaps.\n",
        "6. Slice into 10s windows / 5s hop.\n",
        "7. Label seizure(1)/background(0).\n",
        "8. We call `window_data_with_artifact(...)`, which slides 10 s / 5 s hop windows across the normalized EEG and returns only the windows we want to train on.\n",
        "   - Rules:\n",
        "     - Overlap seizure: KEEP (label=1).\n",
        "     - Overlap artifact (but no seizure) -> DROP.\n",
        "     - Otherwise: KEEP as background (label=0).\n",
        "   - We count:\n",
        "     - how many windows we kept,\n",
        "     - how many were labeled seizure vs background,\n",
        "     - how many artifact-only windows we dropped.\n",
        "\n",
        "Returns:\n",
        "- `X` shape `[N_win, C_curr, T]` in THE RECORDING'S OWN channel order\n",
        "- `y` shape `[N_win]`\n",
        "- `meta` dict with bookkeeping\n",
        "- `debug_preview` dict with windows, labels, fs, channel_names (used later for alignment & QC)"
      ],
      "metadata": {
        "id": "xrZuqH1AGmCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_single_recording_raw(\n",
        "    edf_path: Path,\n",
        "    ann_csv_path: Path,\n",
        "    win_len_s=WIN_LEN_S,\n",
        "    hop_s=HOP_S,\n",
        "    num_taps=NUM_TAPS\n",
        "):\n",
        "\n",
        "\n",
        "    print(f\"INFO ONLY: EDF: {edf_path.as_posix()}\")\n",
        "    print(f\"CSV: {ann_csv_path.as_posix()}\")\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Load raw EDF with MNE\n",
        "    # -------------------------------------------------\n",
        "    raw = mne.io.read_raw_edf(\n",
        "        edf_path.as_posix(),\n",
        "        preload=True,\n",
        "        verbose='ERROR'\n",
        "    )\n",
        "\n",
        "    data = raw.get_data()      # shape [C_raw, N_samples]\n",
        "    ch_names = raw.ch_names    # list of length C_raw\n",
        "    fs_orig = float(raw.info['sfreq'])\n",
        "    n_samp = data.shape[1]\n",
        "\n",
        "    print(f\"fs_orig={fs_orig} Hz  ch={data.shape[0]}  n_samples={n_samp}\")\n",
        "\n",
        "    '''\n",
        "     Check if clip is long enough for filtfilt padding\n",
        "     filtfilt needs ~3*(num_taps-1) samples of margin.\n",
        "     If not long enough, we cannot safely apply the Kaiser filter.\n",
        "     We just skip this recording entirely.\n",
        "    '''\n",
        "    padlen_needed = 3 * (num_taps - 1)\n",
        "    if n_samp <= padlen_needed:\n",
        "        print(f\"SKIP SHORT: len={n_samp/fs_orig:.1f}s \"\n",
        "              f\"(need >{padlen_needed/fs_orig:.1f}s for filtfilt with {num_taps} taps)\")\n",
        "        return (\n",
        "            np.empty((0,0,0), dtype=np.float32),\n",
        "            np.empty((0,), dtype=np.int64),\n",
        "            {\n",
        "                'recording_id': edf_path.stem,\n",
        "                'fs': fs_orig,\n",
        "                'skipped_short': True,\n",
        "                'num_windows': 0,\n",
        "                'num_seizure_windows': 0,\n",
        "                'num_background_windows': 0,\n",
        "                'num_artifact_windows_dropped': 0,\n",
        "                'total_seizure_seconds_in_file': 0.0,\n",
        "                'total_artifact_seconds_in_file': 0.0,\n",
        "                'raw_ch_names': ch_names,\n",
        "                'num_channels_raw': len(ch_names),\n",
        "            },\n",
        "            None,\n",
        "        )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 0.5–40 Hz bandpass using Kaiser FIR with zero-phase filtfilt\n",
        "    # -------------------------------------------------\n",
        "    data_bp = apply_kaiser_bandpass(\n",
        "        data,\n",
        "        fs_orig,\n",
        "        f_lo=0.5,\n",
        "        f_hi=40.0,\n",
        "        num_taps=num_taps,\n",
        "        ripple_db=60.0,\n",
        "    )\n",
        "    print(f\"after bandpass: {data_bp.shape}\")\n",
        "\n",
        "    '''\n",
        "     Downsample if needed\n",
        "     If fs_orig > TARGET_FS (e.g. 500 Hz), we polyphase resample down to 250 Hz\n",
        "     for anti-aliasing + speed.\n",
        "     If fs_orig <= TARGET_FS, we keep as is (no upsampling).\n",
        "    '''\n",
        "    data_ds, fs_proc = maybe_resample_to_250(\n",
        "        data_bp,\n",
        "        fs_orig,\n",
        "        target_fs=TARGET_FS\n",
        "    )\n",
        "    print(f\"after resample check: {data_ds.shape}, fs_proc={fs_proc}\")\n",
        "\n",
        "\n",
        "    # Robust z-score (median / IQR) per channel\n",
        "\n",
        "    data_norm = robust_zscore(data_ds)\n",
        "    print(f\"after robust z-score: {data_norm.shape}\")\n",
        "\n",
        "    '''\n",
        "    Load seizure + artifact intervals from CSV\n",
        "     We will use both to decide which windows to keep.\n",
        "     - seizure windows: always kept, labeled 1\n",
        "     - artifact-only windows: dropped entirely\n",
        "     - clean windows: kept, labeled 0\n",
        "    '''\n",
        "    seizure_intervals = load_seizure_intervals(ann_csv_path)\n",
        "    artifact_intervals = load_artifact_intervals(ann_csv_path)\n",
        "\n",
        "    total_sz_sec = (\n",
        "        sum(e - s for (s, e) in seizure_intervals)\n",
        "        if seizure_intervals else 0.0\n",
        "    )\n",
        "    total_artf_sec = (\n",
        "        sum(e - s for (s, e) in artifact_intervals)\n",
        "        if artifact_intervals else 0.0\n",
        "    )\n",
        "\n",
        "    print(f\"seizure_intervals={seizure_intervals}\")\n",
        "    print(f\"total seizure sec in file: {total_sz_sec:.2f}\")\n",
        "    print(f\"artifact_intervals={artifact_intervals}\")\n",
        "    print(f\"total artifact sec in file: {total_artf_sec:.2f}\")\n",
        "\n",
        "    ''' Sliding windows with artifact-aware policy\n",
        "     window_data_with_artifact() returns:\n",
        "       X  -> kept windows [N_keep, C_curr, T]\n",
        "       y  -> labels [N_keep] (1=seizure, 0=clean background)\n",
        "       t0s -> start times of windows (sec from start)\n",
        "       dropped_art -> how many windows we removed due to artifact-only contamination\n",
        "    '''\n",
        "    X, y, t0s, dropped_art = window_data_with_artifact(\n",
        "        data_norm,\n",
        "        fs_proc,\n",
        "        seizure_intervals,\n",
        "        artifact_intervals,\n",
        "        win_len_s=win_len_s,\n",
        "        hop_s=hop_s,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"=> {X.shape[0]} KEPT windows | \"\n",
        "        f\"sz={int(np.sum(y==1))} | \"\n",
        "        f\"bg={int(np.sum(y==0))} | \"\n",
        "        f\"dropped_artifact={dropped_art}\"\n",
        "    )\n",
        "\n",
        "    '''\n",
        "    Build debug_preview for downstream reference alignment and QC plotting.\n",
        "    NOTE: channel_names here are the ORIGINAL channel order for THIS EDF.\n",
        "    We'll align them to the canonical montage later in build_split().\n",
        "    '''\n",
        "    debug_preview = None\n",
        "    if X.shape[0] > 0:\n",
        "        debug_preview = {\n",
        "            'recording_id': edf_path.stem,\n",
        "            'all_windows': X,        # [N_keep, C_curr, T]\n",
        "            'all_labels': y,         # [N_keep]\n",
        "            'fs': fs_proc,\n",
        "            'channel_names': ch_names,\n",
        "        }\n",
        "\n",
        "  '''\n",
        "     Metadata for analysis and auditing\n",
        "     We'll also store:\n",
        "       - total_artifact_seconds_in_file\n",
        "       - num_artifact_windows_dropped\n",
        "    '''\n",
        "    meta = {\n",
        "        'recording_id': edf_path.stem,\n",
        "        'fs': fs_proc,\n",
        "        'num_windows': int(len(y)),\n",
        "        'num_seizure_windows': int(np.sum(y==1)),\n",
        "        'num_background_windows': int(np.sum(y==0)),\n",
        "        'num_artifact_windows_dropped': int(dropped_art),\n",
        "        'total_seizure_seconds_in_file': total_sz_sec,\n",
        "        'total_artifact_seconds_in_file': total_artf_sec,\n",
        "        'skipped_short': False,\n",
        "        'raw_ch_names': ch_names,\n",
        "        'num_channels_raw': data_norm.shape[0],\n",
        "    }\n",
        "\n",
        "    return X, y, meta, debug_preview"
      ],
      "metadata": {
        "id": "JGmsmgfcG-0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a split (`train`, `dev`, or `eval`)\n",
        "\n",
        "Algorithm:\n",
        "1. Collect all EDF files under the chosen split dir.\n",
        "2. If `GROUP_MODE == 'subject'`, group EDFs by subject folder and cap at `MAX_SUBJECTS`.\n",
        "3. For each EDF:\n",
        "   - Preprocess.\n",
        "   - The first usable recording defines `ref_ch_names` (our learned canonical montage order).\n",
        "   - For each window in that recording (and all later recordings), align channel order to `ref_ch_names` using `align_to_reference`.\n",
        "   - Compute coverage_ratio for that recording (fraction of ref channels actually present).\n",
        "   - If coverage_ratio < `min_coverage` (default 0.3 = 30%), skip that recording so we don't add mostly-zero tensors.\n",
        "4. Concatenate aligned windows across recordings -> one big `X_split`, `y_split`.\n",
        "5. Pick the best debug candidate for QC plotting (prefer seizure-heavy recording)."
      ],
      "metadata": {
        "id": "zf3iv0bQHnlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_subject_id(split_root: Path, edf_path: Path) -> str:\n",
        "    rel = edf_path.relative_to(split_root)\n",
        "    parts = rel.parts\n",
        "    if len(parts) < 2:\n",
        "        return \"unknown\"\n",
        "    return parts[0]\n",
        "\n",
        "def build_split(split_root: Path,\n",
        "                group_mode=GROUP_MODE,\n",
        "                max_subjects=MAX_SUBJECTS,\n",
        "                max_recordings=MAX_RECORDINGS,\n",
        "                win_len_s=WIN_LEN_S,\n",
        "                hop_s=HOP_S,\n",
        "                num_taps=NUM_TAPS,\n",
        "                min_coverage=0.3):\n",
        "    \"\"\"\n",
        "    min_coverage:\n",
        "      If a recording can't map at least this fraction of the reference channels,\n",
        "      we skip it (to avoid mostly-zero data).\n",
        "    \"\"\"\n",
        "\n",
        "    split_root = Path(split_root)\n",
        "    print(f\"\\nINFO ONLY ==== Processing split: {split_root.as_posix()} ====\")\n",
        "\n",
        "    edf_files = list(split_root.rglob(\"*.edf\"))\n",
        "    print(f\"INFO ONLY Found {len(edf_files)} EDF files under {split_root.as_posix()}\")\n",
        "\n",
        "    X_all_list = []\n",
        "    y_all_list = []\n",
        "    metas = []\n",
        "    debug_candidates = []\n",
        "\n",
        "    ref_ch_names = None  # learned canonical order from the first usable recording\n",
        "\n",
        "    def handle_edf_list(edf_iter):\n",
        "        nonlocal ref_ch_names\n",
        "        count_used = 0\n",
        "        for edf_path in edf_iter:\n",
        "            if max_recordings is not None and count_used >= max_recordings:\n",
        "                break\n",
        "\n",
        "            ann_path = find_best_annotation_for_edf(edf_path)\n",
        "            if ann_path is None:\n",
        "                print(\"SKIP NO CSV\", edf_path.as_posix(),\n",
        "                      \"CSV candidates:\", [c.name for c in edf_path.parent.glob(\"*.csv\")])\n",
        "                continue\n",
        "\n",
        "            X_rec, y_rec, meta_rec, dbg = preprocess_single_recording_raw(\n",
        "                edf_path,\n",
        "                ann_path,\n",
        "                win_len_s=win_len_s,\n",
        "                hop_s=hop_s,\n",
        "                num_taps=num_taps,\n",
        "            )\n",
        "\n",
        "            if X_rec.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            # first usable recording defines reference channel layout\n",
        "            if ref_ch_names is None:\n",
        "                if dbg is None:\n",
        "                    continue\n",
        "                ref_ch_names = list(dbg['channel_names'])\n",
        "                print(f\"INFO ONLY reference channel layout set from {edf_path.stem}:\")\n",
        "                print(ref_ch_names)\n",
        "\n",
        "            # align every window in this recording to ref_ch_names\n",
        "            aligned_list = []\n",
        "            coverages = []\n",
        "            for w in X_rec:  # w: [C_curr, T]\n",
        "                aligned_w, cov = align_to_reference(\n",
        "                    w,\n",
        "                    dbg['channel_names'],\n",
        "                    ref_ch_names\n",
        "                )\n",
        "                aligned_list.append(aligned_w)\n",
        "                coverages.append(cov)\n",
        "\n",
        "            X_rec_aligned = np.stack(aligned_list, axis=0).astype(np.float32)\n",
        "            avg_cov = float(np.mean(coverages)) if len(coverages) else 0.0\n",
        "            print(f\"coverage for {edf_path.stem}: {avg_cov*100:.1f}%\")\n",
        "\n",
        "            # skip low-coverage recordings (prevents mostly-zero tensors)\n",
        "            if avg_cov < min_coverage:\n",
        "                print(f\"SKIP LOW COVERAGE {edf_path.stem} \"\n",
        "                      f\"({avg_cov*100:.1f}% < {min_coverage*100:.1f}%)\")\n",
        "                continue\n",
        "\n",
        "            # keep it\n",
        "            X_all_list.append(X_rec_aligned)\n",
        "            y_all_list.append(y_rec)\n",
        "\n",
        "            meta_with_cov = dict(meta_rec)\n",
        "            meta_with_cov['coverage_ratio'] = avg_cov\n",
        "            metas.append(meta_with_cov)\n",
        "            count_used += 1\n",
        "\n",
        "            # candidate for QC plotting\n",
        "            if dbg is not None:\n",
        "                debug_candidates.append({\n",
        "                    'recording_id': dbg['recording_id'],\n",
        "                    'all_windows': X_rec_aligned,\n",
        "                    'all_labels': y_rec,\n",
        "                    'fs': dbg['fs'],\n",
        "                    'channel_names': ref_ch_names,\n",
        "                })\n",
        "\n",
        "    if group_mode == \"recording\":\n",
        "        handle_edf_list(edf_files)\n",
        "    else:\n",
        "        subj_to_edfs = {}\n",
        "        for edf_path in edf_files:\n",
        "            subj_id = _extract_subject_id(split_root, edf_path)\n",
        "            subj_to_edfs.setdefault(subj_id, []).append(edf_path)\n",
        "\n",
        "        subjects_sorted = sorted(subj_to_edfs.keys())[:max_subjects]\n",
        "        print(f\"INFO ONLY Subjects used (cap={max_subjects}): {subjects_sorted}\")\n",
        "\n",
        "        for subj_id in subjects_sorted:\n",
        "            recs_this_subj = subj_to_edfs[subj_id]\n",
        "            handle_edf_list(recs_this_subj)\n",
        "\n",
        "    # concatenate aligned windows from all accepted recordings\n",
        "    if len(X_all_list) == 0:\n",
        "        X_all = np.empty((0,0,0), dtype=np.float32)\n",
        "        y_all = np.empty((0,), dtype=np.int64)\n",
        "    else:\n",
        "        X_all = np.concatenate(X_all_list, axis=0)\n",
        "        y_all = np.concatenate(y_all_list, axis=0)\n",
        "\n",
        "    # pick best debug candidate for QC plotting (prefer more seizure windows)\n",
        "    best_dbg = None\n",
        "    best_sz = -1\n",
        "    best_total = -1\n",
        "    for dbg in debug_candidates:\n",
        "        labels = dbg.get('all_labels', None)\n",
        "        wins   = dbg.get('all_windows', None)\n",
        "        if labels is None or wins is None:\n",
        "            continue\n",
        "\n",
        "        sz_count = int(np.sum(labels == 1))\n",
        "        total_w  = int(len(labels))\n",
        "        if sz_count > best_sz:\n",
        "            best_dbg = dbg\n",
        "            best_sz = sz_count\n",
        "            best_total = total_w\n",
        "        elif sz_count == best_sz and total_w > best_total:\n",
        "            best_dbg = dbg\n",
        "            best_total = total_w\n",
        "\n",
        "    return X_all, y_all, metas, best_dbg"
      ],
      "metadata": {
        "id": "ScFTP7s0HzxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run preprocessing for one split\n",
        "\n",
        "Set `SPLIT` to `'train'`, `'dev'`, or `'eval'`.\n",
        "\n",
        "1. Call `build_split`.\n",
        "2. Print dataset stats and class balance.\n",
        "3. Report the debug candidate we'll use for QC plotting."
      ],
      "metadata": {
        "id": "sQ5hLOJiIPmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPLIT = \"train\"  # change to \"dev\" or \"eval\" as needed\n",
        "\n",
        "split_dir = ROOT_DIR / SPLIT\n",
        "X_split, y_split, meta_split, dbg_split = build_split(\n",
        "    split_dir,\n",
        "    group_mode=GROUP_MODE,\n",
        "    max_subjects=MAX_SUBJECTS,\n",
        "    max_recordings=MAX_RECORDINGS,\n",
        "    win_len_s=WIN_LEN_S,\n",
        "    hop_s=HOP_S,\n",
        "    num_taps=NUM_TAPS,\n",
        "    min_coverage=0.3,  # skip recordings with <30% channel match\n",
        ")\n",
        "\n",
        "print(\"\\n=== SPLIT SUMMARY ===\")\n",
        "print(\"X_split shape:\", getattr(X_split, 'shape', None))\n",
        "print(\"y_split shape:\", getattr(y_split, 'shape', None))\n",
        "\n",
        "total_windows = len(y_split)\n",
        "num_bg = int(np.sum(y_split == 0)) if total_windows > 0 else 0\n",
        "num_sz = int(np.sum(y_split == 1)) if total_windows > 0 else 0\n",
        "pct_bg = (100.0 * num_bg / total_windows) if total_windows > 0 else 0.0\n",
        "pct_sz = (100.0 * num_sz / total_windows) if total_windows > 0 else 0.0\n",
        "\n",
        "print(f\"Total windows: {total_windows}\")\n",
        "print(f\"Background windows: {num_bg} ({pct_bg:.2f}%)\")\n",
        "print(f\"Seizure windows:    {num_sz} ({pct_sz:.2f}%)\")\n",
        "print(f\"Class ratio (bg : sz) ~ {num_bg}:{num_sz}\")\n",
        "\n",
        "if dbg_split is not None:\n",
        "    lbls_dbg = dbg_split.get('all_labels', None)\n",
        "    if lbls_dbg is not None:\n",
        "        print(\"Best debug recording:\", dbg_split.get('recording_id','n/a'))\n",
        "        print(\"Seizure windows in debug rec:\", int(np.sum(lbls_dbg==1)))\n",
        "else:\n",
        "    print('WARNING: No debug candidate available to plot.')"
      ],
      "metadata": {
        "id": "ZMzO_fo6IWZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QC plot\n",
        "\n",
        "We pick an informative EEG window from `dbg_split`:\n",
        "1. Prefer seizure windows first (label==1), otherwise background.\n",
        "2. Within that window, pick the channel with the highest std (so we avoid flat zero-fill rows).\n",
        "3. Plot that channel's robust z-scored signal.\n",
        "We save this as `qc_window.png` in `OUT_DIR`."
      ],
      "metadata": {
        "id": "jL5nGnIyIdpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_informative_window(dbg_dict, motion_thresh=1e-6):\n",
        "    if dbg_dict is None:\n",
        "        return None, None\n",
        "\n",
        "    X_all = dbg_dict.get('all_windows', None)   # [N_win, C_ref, T]\n",
        "    y_all = dbg_dict.get('all_labels', None)    # [N_win]\n",
        "    if X_all is None or y_all is None or len(y_all) == 0:\n",
        "        return None, None\n",
        "\n",
        "    seiz_idx = np.where(y_all == 1)[0]\n",
        "    non_idx  = np.where(y_all == 0)[0]\n",
        "    order = list(seiz_idx) + list(non_idx)\n",
        "\n",
        "    for w_i in order:\n",
        "        win = X_all[w_i]  # [C_ref, T]\n",
        "        if win.size == 0:\n",
        "            continue\n",
        "        ch_std = np.std(win, axis=1)\n",
        "        ch_i = int(np.argmax(ch_std))\n",
        "        if ch_std[ch_i] > motion_thresh:\n",
        "            return w_i, ch_i\n",
        "\n",
        "    return None, None\n",
        "\n",
        "if dbg_split is not None:\n",
        "    w_i, ch_i = pick_informative_window(dbg_split, motion_thresh=1e-6)\n",
        "    if w_i is not None and ch_i is not None:\n",
        "        X_all = dbg_split['all_windows']\n",
        "        y_all = dbg_split['all_labels']\n",
        "        fs_dbg = dbg_split['fs']\n",
        "\n",
        "        sig = X_all[w_i, ch_i, :]\n",
        "        label_dbg = int(y_all[w_i])\n",
        "        t_axis = np.arange(sig.shape[0]) / fs_dbg\n",
        "\n",
        "        print(f\"QC plotting window {w_i} label={label_dbg} channel {ch_i}\")\n",
        "        print(\"     std:\", float(np.std(sig)),\n",
        "              \"min:\", float(sig.min()),\n",
        "              \"max:\", float(sig.max()))\n",
        "\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.plot(t_axis, sig)\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Robust z-scored amplitude')\n",
        "        plt.title(f'QC window w={w_i} ch={ch_i} label={label_dbg}')\n",
        "        plt.grid(True)\n",
        "        plt.autoscale()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        qc_path = (OUT_DIR / 'qc_window.png').as_posix()\n",
        "        plt.savefig(qc_path, dpi=200)\n",
        "        print('INFO ONLY Saved QC plot to', qc_path)\n",
        "    else:\n",
        "        print('INFO ONLY Could not find any non-flat channel/window to plot.')\n",
        "else:\n",
        "    print('INFO ONLY Skipping QC plot; no debug recording found.')"
      ],
      "metadata": {
        "id": "1VP7s4ArIkrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save dataset + class balance\n",
        "\n",
        "Save:\n",
        "- `split_windows.npz` with:\n",
        "  - `X`: all aligned windows for this split `[N_total, C_ref, T]`\n",
        "  - `y`: labels `[N_total]`\n",
        "  - `meta`: list of per-recording metadata dicts (coverage ratio, seizure sec, etc.)\n",
        "- `class_balance.png`: simple seizure vs background bar chart."
      ],
      "metadata": {
        "id": "MrWBzMw2I32P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if X_split is not None and y_split is not None:\n",
        "    out_npz_path = OUT_DIR / 'split_windows.npz'\n",
        "    np.savez_compressed(out_npz_path.as_posix(), X=X_split, y=y_split, meta=meta_split)\n",
        "    print('INFO ONLY Saved NPZ to', out_npz_path.as_posix())\n",
        "\n",
        "    total_windows = len(y_split)\n",
        "    num_bg = int(np.sum(y_split == 0)) if total_windows > 0 else 0\n",
        "    num_sz = int(np.sum(y_split == 1)) if total_windows > 0 else 0\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.bar(['background','seizure'], [num_bg, num_sz])\n",
        "    plt.ylabel('Window count')\n",
        "    plt.title('Class distribution in this split')\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    dist_path = (OUT_DIR / 'class_balance.png').as_posix()\n",
        "    plt.savefig(dist_path, dpi=200)\n",
        "    print('INFO ONLY Saved class balance bar chart to', dist_path)\n",
        "else:\n",
        "    print('WARNING: Nothing to save / plot class balance (no data).')"
      ],
      "metadata": {
        "id": "iFweLHhFI-vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Point this to the OUT_DIR\n",
        "LOAD_DIR = Path(\"Enter Output folder here\").expanduser().resolve()\n",
        "npz_path = LOAD_DIR / \"split_windows.npz\"\n",
        "\n",
        "print(\"Loading:\", npz_path.as_posix())\n",
        "data = np.load(npz_path, allow_pickle=True)\n",
        "\n",
        "X = data[\"X\"] # shape [N, C_ref, T]\n",
        "y = data[\"y\"] # shape [N]\n",
        "meta_list = data[\"meta\"] # this is an array of Python dicts (dtype=object)\n",
        "\n",
        "print(\"\\n=== DATASET SHAPES ===\")\n",
        "print(\"X.shape:\", X.shape) # (N_win, C_ref, T)\n",
        "print(\"y.shape:\", y.shape) # (N_win,)\n",
        "N, C_ref, T = X.shape if X.size > 0 else (0,0,0)\n",
        "print(f\"N windows = {N}\")\n",
        "print(f\"C_ref (channels after smart canonical) = {C_ref}\")\n",
        "print(f\"T samples per window = {T}\")\n",
        "\n",
        "\n",
        "# Class balance\n",
        "\n",
        "num_bg = int(np.sum(y == 0))\n",
        "num_sz = int(np.sum(y == 1))\n",
        "total = len(y)\n",
        "\n",
        "pct_bg = 100.0 * num_bg / total if total > 0 else 0.0\n",
        "pct_sz = 100.0 * num_sz / total if total > 0 else 0.0\n",
        "\n",
        "print(\"\\n=== CLASS BALANCE ===\")\n",
        "print(f\"background (0): {num_bg} ({pct_bg:.2f} %)\")\n",
        "print(f\"seizure (1): {num_sz} ({pct_sz:.2f} %)\")\n",
        "print(f\"bg : sz ratio ~ {num_bg}:{num_sz}\")\n",
        "\n",
        "'''\n",
        " Metadata summary\n",
        " Each element in meta_list corresponds to one recording:\n",
        " - 'recording_id'\n",
        " - 'fs'\n",
        " - 'num_windows'\n",
        " - 'num_seizure_windows'\n",
        " - 'num_background_windows'\n",
        " - 'total_seizure_seconds_in_file'\n",
        " - 'coverage_ratio'\n",
        " - etc.\n",
        "'''\n",
        "\n",
        "print(\"\\n=== PER-RECORDING META (first 10 rows) ===\")\n",
        "for i, m in enumerate(meta_list[:10]):\n",
        "    print(f\"[{i}] rec_id={m.get('recording_id','?')}\")\n",
        "    print(f\" fs_proc={m.get('fs','?')} Hz\")\n",
        "    print(f\" windows={m.get('num_windows','?')} \"\n",
        "          f\"seiz_win={m.get('num_seizure_windows','?')} \"\n",
        "          f\"bg_win={m.get('num_background_windows','?')}\")\n",
        "    print(f\" seizure_time_in_file={m.get('total_seizure_seconds_in_file','?')} sec\")\n",
        "    cov = m.get('coverage_ratio', None)\n",
        "    if cov is not None:\n",
        "        print(f\" coverage={cov*100:.1f}%\")\n",
        "    else:\n",
        "        print(\" coverage=N/A (this was the reference file)\")\n",
        "    print(\"\")\n",
        "\n",
        "def plot_random_window(X, y, fs, target_label=1, motion_thresh=1e-6):\n",
        "    idxs = np.where(y == target_label)[0]\n",
        "    if len(idxs) == 0:\n",
        "        print(f\"[PLOT] No windows with label={target_label}\")\n",
        "        return\n",
        "\n",
        "    w_idx = int(random.choice(idxs))\n",
        "    win = X[w_idx] # shape [C_ref, T]\n",
        "\n",
        "    ch_std = np.std(win, axis=1)\n",
        "    ch_i = int(np.argmax(ch_std))\n",
        "    if ch_std[ch_i] <= motion_thresh:\n",
        "        print(f\"[PLOT] Chosen window {w_idx} is too flat (std={ch_std[ch_i]:.2e}). Trying background instead?\")\n",
        "        # we could try fallback logic here, but let's just show anyway\n",
        "    sig = win[ch_i, :]\n",
        "    t = np.arange(sig.shape[0]) / fs\n",
        "\n",
        "    print(f\"[PLOT] window {w_idx}, label={target_label}, chosen channel idx={ch_i}, std={ch_std[ch_i]:.3f}\")\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(t, sig)\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Robust z-scored amplitude\")\n",
        "    plt.title(f\"Random window {w_idx} (label={target_label}) ch={ch_i}\")\n",
        "    plt.grid(True)\n",
        "    plt.autoscale()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "'''\n",
        " We don't store fs directly in X/y, but all aligned data in this split\n",
        " share the same fs because we either downsampled to TARGET_FS (250)\n",
        " or left lower fs alone. The metadata for any surviving recording\n",
        " will tell us. We'll just grab the first one.\n",
        "'''\n",
        "if len(meta_list) > 0:\n",
        "    fs_est = meta_list[0].get(\"fs\", TARGET_FS)\n",
        "else:\n",
        "    fs_est = TARGET_FS\n",
        "print(f\"\\nAssumed sampling rate for plotting: {fs_est} Hz\")\n",
        "\n",
        "\n",
        "# Plot example seizure and background windows\n",
        "\n",
        "plot_random_window(X, y, fs_est, target_label=1) # seizure\n",
        "plot_random_window(X, y, fs_est, target_label=0) # background"
      ],
      "metadata": {
        "id": "H4GUt4fXJBYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}